{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a800da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentals of machine learning using Python \n",
    "## Evaluation\n",
    "\n",
    "***\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d413104",
   "metadata": {},
   "source": [
    "## What is model evaluation?\n",
    "\n",
    "* Model evaluation is the process of using different evaluation metrics to understand a machine learning model’s performance, as well as its strengths and weaknesses.\n",
    "* Model evaluation is important to assess the efficacy of a model during initial research phases, and it also plays a role in model monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f0a35",
   "metadata": {},
   "source": [
    "## Evaluating the classifier\n",
    "\n",
    "* The most popular metrics for measuring classification performance include accuracy, precision, confusion matrix, log-loss, and AUC (area under the ROC curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e6d6b",
   "metadata": {},
   "source": [
    "#### Classification Accuracy\n",
    "\n",
    "* It is the simplest out of all the methods of evaluating the accuracy, and the most commonly used.\n",
    "* It is simply the number of correct predictions divided by all predictions or a ratio of correct predictions to total predictions.\n",
    "* While it can give you a quick idea of how your classifier is performing, it is best used when the number of observations/examples in each class is roughly equivalent.\n",
    "* Because this doesn't happen very often, you're probably better off using another metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2e494",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "* It measures the proportion of predicted Positives that are truly Positive.\n",
    "* Precision is a good choice of evaluation metrics when you want to be very sure of your prediction.\n",
    "* For example, if you are building a system to predict whether to decrease the credit limit on a particular account, you want to be very sure about the prediction or it may result in customer dissatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44204e1",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "* The confusion matrix (or confusion table) shows a more detailed breakdown of correct and incorrect classifications for each class.\n",
    "* Using a confusion matrix is useful when you want to understand the distinction between classes, particularly when the cost of misclassification might differ for the two classes, or you have a lot more test data on one class than the other.\n",
    "* For example, the consequences of making a false positive or false negative in a cancer diagnosis are very different.\n",
    "\n",
    "<img src=\"img/confusion-matrix.jpeg\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8624c5",
   "metadata": {},
   "source": [
    "#### Logarithmic Loss\n",
    "\n",
    "* Logarithmic Loss, or LogLoss, essentially evaluates how confident the classifier is about its predictions.\n",
    "* LogLoss returns probabilities for membership of an example in a given class, summing them together to give a representation of the classifier's general confidence.\n",
    "* The value for predictions runs from 1 to 0, with 1 being completely confident and 0 being no confidence.\n",
    "* The loss, or overall lack of confidence, is returned as a negative number with 0 representing a perfect classifier, so smaller values are better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f94ce",
   "metadata": {},
   "source": [
    "#### Area Under ROC Curve (AUC)\n",
    "\n",
    "* This is a metric used only for binary classification problems.\n",
    "* The area under the curve represents the model's ability to properly discriminate between negative and positive examples, between one class or another.\n",
    "* A 1.0, all of the area falling under the curve, represents a perfect classifier.\n",
    "* This means that an AUC of 0.5 is basically as good as randomly guessing.\n",
    "* The ROC curve is calculated with regards to sensitivity (true positive rate/recall) and specificity (true negative rate).\n",
    "\n",
    "<img src=\"img/auc.png\" style=\"width:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c0a99",
   "metadata": {},
   "source": [
    "#### Classification Report\n",
    "\n",
    "* The classification report is a Scikit-Learn built in metric created especially for classification problems.\n",
    "* Using the classification report can give you a quick intuition of how your model is performing.\n",
    "* Recall pits the number of examples your model labeled as Class A (some given class) against the total number of examples of Class A, and this is represented in the report.\n",
    "* The report also returns prediction and f1-score.\n",
    "* Precision is the percentage of examples your model labeled as Class A which actually belonged to Class A (true positives against false positives), and f1-score is an average of precision and recall.\n",
    "\n",
    "<img src=\"img/classification-report.png\" style=\"width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22009ff8",
   "metadata": {},
   "source": [
    "## Example of classifier evaluation\n",
    "\n",
    "Comparison of the quality of svm and logistic regression classifiers when applied to `data/pima-indians-diabetes.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd2d79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and train classifiers\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dataset = pd.read_csv(\"data/pima-indians-diabetes.csv\")\n",
    "X = dataset.iloc[:,0:8].values\n",
    "y = dataset.iloc[:,8]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "clf_lr = LogisticRegression(random_state=0)\n",
    "clf_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6289646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set evaluation\n",
      "[[313  81]\n",
      " [ 36 107]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84       394\n",
      "           1       0.57      0.75      0.65       143\n",
      "\n",
      "    accuracy                           0.78       537\n",
      "   macro avg       0.73      0.77      0.74       537\n",
      "weighted avg       0.81      0.78      0.79       537\n",
      "\n",
      "\n",
      "Test set evaluation\n",
      "[[123  30]\n",
      " [ 28  50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81       153\n",
      "           1       0.62      0.64      0.63        78\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.72      0.72      0.72       231\n",
      "weighted avg       0.75      0.75      0.75       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions and evaluate - svm classsifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_predictions_svm = clf_svm.predict(X_train)\n",
    "test_predictions_svm = clf_svm.predict(X_test)\n",
    "\n",
    "print(\"Train set evaluation\")\n",
    "print(confusion_matrix(train_predictions_svm, y_train))\n",
    "print(classification_report(train_predictions_svm, y_train))\n",
    "\n",
    "print()\n",
    "print(\"Test set evaluation\")\n",
    "print(confusion_matrix(test_predictions_svm, y_test))\n",
    "print(classification_report(test_predictions_svm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec87304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set evaluation\n",
      "[[311  79]\n",
      " [ 38 109]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84       390\n",
      "           1       0.58      0.74      0.65       147\n",
      "\n",
      "    accuracy                           0.78       537\n",
      "   macro avg       0.74      0.77      0.75       537\n",
      "weighted avg       0.81      0.78      0.79       537\n",
      "\n",
      "\n",
      "Test set evaluation\n",
      "[[120  30]\n",
      " [ 31  50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80       150\n",
      "           1       0.62      0.62      0.62        81\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.71      0.71      0.71       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions and evaluate - logistic regression classsifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_predictions_lr = clf_lr.predict(X_train)\n",
    "test_predictions_lr = clf_lr.predict(X_test)\n",
    "\n",
    "print(\"Train set evaluation\")\n",
    "print(confusion_matrix(train_predictions_lr, y_train))\n",
    "print(classification_report(train_predictions_lr, y_train))\n",
    "\n",
    "print()\n",
    "print(\"Test set evaluation\")\n",
    "print(confusion_matrix(test_predictions_lr, y_test))\n",
    "print(classification_report(test_predictions_lr, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930f6f1",
   "metadata": {},
   "source": [
    "By observing the values of the evaluation parameters, we can conclude that both classifiers are of similar quality. The differences between them are relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f71107",
   "metadata": {},
   "source": [
    "## Evaluating the regressor\n",
    "\n",
    "* For regression models, three evaluation metrics are mainly used: R Square, Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd594ea",
   "metadata": {},
   "source": [
    "#### R-Squared\n",
    "\n",
    "* R-Squared is a statistical measure of fit that indicates how much variation of a dependent variable is explained by the independent variable(s) in a regression model.\n",
    "* In investing, R-squared is generally interpreted as the percentage of a fund or security's movements that can be explained by movements in a benchmark index.\n",
    "* An R-squared of 100% means that all movements of a security (or other dependent variables) are completely explained by movements in the index (or the independent variable(s) you are interested in).\n",
    "\n",
    "$R^2 = 1 − \\frac{Unexplained Variation}{Total Variation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1437ca8",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "* When we subtract the predicted values from the actual values, obtaining the errors, sum the absolute values of those errors and get their mean.\n",
    "* This metric gives a notion of the overall error for each prediction of the model, the smaller (closer to 0) the better.\n",
    "\n",
    "$mae = \\frac{1}{n} \\sum_{i=1}^{n}|Actual−Predicted|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3683af3",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "* It is similar to the MAE metric, but it squares the absolute values of the errors.\n",
    "* Also, as with MAE, the smaller, or closer to 0, the better.\n",
    "* The MSE value is squared so as to make large errors even larger.\n",
    "* One thing to pay close attention to, it that it is usually a hard metric to interpret due to the size of its values and of the fact that they aren't in the same scale of the data.\n",
    "\n",
    "$mse = \\frac{1}{n} \\sum_{i=1}^{n}(Actual−Predicted)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42380f74",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "* Tries to solve the interpretation problem raised with the MSE by getting the square root of its final value, so as to scale it back to the same units of the data.\n",
    "* It is easier to interpret and good when we need to display or show the actual value of the data with the error.\n",
    "* It shows how much the data may vary, so, if we have an RMSE of 4.35, our model can make an error either because it added 4.35 to the actual value, or needed 4.35 to get to the actual value.\n",
    "* The closer to 0, the better as well.\n",
    "\n",
    "$rmse = \\sqrt{mse} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(Actual−Predicted)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a087311",
   "metadata": {},
   "source": [
    "## Example of regressor evaluation\n",
    "\n",
    "Determination of evaluation parameters for a linear regression model built on dataset `data/petrol_consumption.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a25e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset and train regressor\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "petrol_dataset = pd.read_csv(\"data/petrol_consumption.csv\")\n",
    "\n",
    "X = petrol_dataset[['Average_income', 'Paved_Highways', 'Population_Driver_licence(%)', 'Petrol_tax']]\n",
    "y = petrol_dataset['Petrol_Consumption']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e3beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set evaluation\n",
      "r2 = 0.7068781342155135\n",
      "mae = 49.377699313187556\n",
      "mse = 4015.2628907647795\n",
      "rmse =  63.366102063838355\n",
      "\n",
      "Test set evaluation\n",
      "r2 = 0.3913664001430558\n",
      "mae = 53.46854128290795\n",
      "mse = 4083.2558717442553\n",
      "rmse =  63.900358932828034\n"
     ]
    }
   ],
   "source": [
    "# make predictions and evaluation\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "predictions_train = regressor.predict(X_train)\n",
    "predictions_test = regressor.predict(X_test)\n",
    "\n",
    "print(\"Train set evaluation\")\n",
    "r2 = r2_score(y_train, predictions_train)\n",
    "mae = mean_absolute_error(y_train, predictions_train)\n",
    "mse = mean_squared_error(y_train, predictions_train)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"r2 =\", r2)\n",
    "print(\"mae =\", mae)\n",
    "print(\"mse =\", mse)\n",
    "print(\"rmse = \", rmse)\n",
    "\n",
    "print()\n",
    "print(\"Test set evaluation\")\n",
    "r2 = r2_score(y_test, predictions_test)\n",
    "mae = mean_absolute_error(y_test, predictions_test)\n",
    "mse = mean_squared_error(y_test, predictions_test)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"r2 =\", r2)\n",
    "print(\"mae =\", mae)\n",
    "print(\"mse =\", mse)\n",
    "print(\"rmse = \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62824c78",
   "metadata": {},
   "source": [
    "Parameter values are slightly better for the training set than for the test set. However, the difference is not significantly large, which may indicate that the resulting model has satisfactory generalisation properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07d686",
   "metadata": {},
   "source": [
    "## --- Exercise ---\n",
    "\n",
    "Find the best classifier for the `data/chronic_kidney_disease.csv` dataset. Evaluate on 20% of the test portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98688370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bp</th>\n",
       "      <th>Sg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Su</th>\n",
       "      <th>Rbc</th>\n",
       "      <th>Bu</th>\n",
       "      <th>Sc</th>\n",
       "      <th>Sod</th>\n",
       "      <th>Pot</th>\n",
       "      <th>Hemo</th>\n",
       "      <th>Wbcc</th>\n",
       "      <th>Rbcc</th>\n",
       "      <th>Htn</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>137.53</td>\n",
       "      <td>4.63</td>\n",
       "      <td>15.4</td>\n",
       "      <td>7800.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>137.53</td>\n",
       "      <td>4.63</td>\n",
       "      <td>11.3</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>137.53</td>\n",
       "      <td>4.63</td>\n",
       "      <td>9.6</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>111.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>3.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>137.53</td>\n",
       "      <td>4.63</td>\n",
       "      <td>11.6</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bp     Sg   Al   Su  Rbc    Bu   Sc     Sod   Pot  Hemo    Wbcc  Rbcc  \\\n",
       "0  80.0  1.020  1.0  0.0  1.0  36.0  1.2  137.53  4.63  15.4  7800.0  5.20   \n",
       "1  50.0  1.020  4.0  0.0  1.0  18.0  0.8  137.53  4.63  11.3  6000.0  4.71   \n",
       "2  80.0  1.010  2.0  3.0  1.0  53.0  1.8  137.53  4.63   9.6  7500.0  4.71   \n",
       "3  70.0  1.005  4.0  0.0  1.0  56.0  3.8  111.00  2.50  11.2  6700.0  3.90   \n",
       "4  80.0  1.010  2.0  0.0  1.0  26.0  1.4  137.53  4.63  11.6  7300.0  4.60   \n",
       "\n",
       "   Htn  Class  \n",
       "0  1.0      1  \n",
       "1  0.0      1  \n",
       "2  0.0      1  \n",
       "3  1.0      1  \n",
       "4  0.0      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"data/chronic_kidney_disease.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
